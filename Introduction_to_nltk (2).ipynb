{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to nltk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import the natural language toolkit"
      ],
      "metadata": {
        "id": "WhCEv5s8wTau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdi1IQbDwM-I"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import all associated modules and libraries in nltk"
      ],
      "metadata": {
        "id": "eunOXqmrwxGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import *"
      ],
      "metadata": {
        "id": "4dskuf1qwvxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentense tokenize"
      ],
      "metadata": {
        "id": "GQbc-JjSw-xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxIxm1L1xidN",
        "outputId": "0005ec7c-468f-4b24-d854-dfa0d5ffe507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello class how are you today. I hope you had a wonderful weekend.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print (tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs4y_nI6w9hP",
        "outputId": "9b287013-1d6b-4047-c461-30e99c91df01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'class', 'how', 'are', 'you', 'today', '.', 'I', 'hope', 'you', 'had', 'a', 'wonderful', 'weekend', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words tokenize"
      ],
      "metadata": {
        "id": "sPA6mSTxxsA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_tokenize = word_tokenize(text)\n",
        "print (words_tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vYsO4gRxq5O",
        "outputId": "e9f0c4c6-2337-4d09-a6fe-314189e300a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'class', 'how', 'are', 'you', 'today', '.', 'I', 'hope', 'you', 'had', 'a', 'wonderful', 'weekend', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "YQDxMR7uUitV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLMRsfXkUlBN",
        "outputId": "1c9db4c6-7bf8-40ce-de63-08880a1d7777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words"
      ],
      "metadata": {
        "id": "C5pR7zrsyFo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"This is an example sentense to illustrate how to use the tokenization in stop words\"\n",
        "stop_words = stopwords.words('english')\n",
        "words_token = word_tokenize(example)\n",
        "filtered_words = []\n",
        "\n",
        "for w in words_token:\n",
        "    if w not in stop_words:\n",
        "        filtered_words.append(w)\n",
        "\n",
        "print(words_token)\n",
        "print()\n",
        "print(filtered_words)  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxEApjBayElJ",
        "outputId": "d94a36e4-bb1c-40d9-c877-c736c571f3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentense', 'to', 'illustrate', 'how', 'to', 'use', 'the', 'tokenization', 'in', 'stop', 'words']\n",
            "\n",
            "['This', 'example', 'sentense', 'illustrate', 'use', 'tokenization', 'stop', 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming technique looks at the form of the word. It means after applying Stemming, we will always get a valid word. whereas Lemmatization technique only looks at the meaning of the word."
      ],
      "metadata": {
        "id": "0Rr7KayV1mW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "example = ['python', 'pythoned', 'pythonized','pythonation']\n",
        "for w in example:\n",
        "  print (ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTYfMJGMya9s",
        "outputId": "5900e135-217b-48b7-8262-4e1ef5f42782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "7d-EaTR92blI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnKbdbLv4C9H",
        "outputId": "d65b24d5-97f1-405e-d013-ebe53ea928d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"bulls\"))\n",
        "print(lemmatizer.lemmatize(\"pythonly\"))\n",
        "print(lemmatizer.lemmatize('goose'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDUvbluU3akW",
        "outputId": "2d5d7385-800f-4a2a-c335-347d53dc73d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "bull\n",
            "pythonly\n",
            "goose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging"
      ],
      "metadata": {
        "id": "bm-0Tn884WvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6g1_Yz44srb",
        "outputId": "282d0928-f44c-4033-ca91-02bf284587d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = word_tokenize (\"hello children my name is peter and my brother is called kamau\")\n",
        "nltk.pos_tag(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKBc5IuY4TFA",
        "outputId": "1d43ed2d-2526-4acc-95c4-19cb39d30619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', 'NN'),\n",
              " ('children', 'NNS'),\n",
              " ('my', 'PRP$'),\n",
              " ('name', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('peter', 'RB'),\n",
              " ('and', 'CC'),\n",
              " ('my', 'PRP$'),\n",
              " ('brother', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('called', 'VBN'),\n",
              " ('kamau', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet"
      ],
      "metadata": {
        "id": "reJAzECK5COM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n",
        "\n",
        "Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. Synset instances are the groupings of synonymous words that express the same concept. Some of the words have only one Synset and some have several."
      ],
      "metadata": {
        "id": "3LWpXXtj5DGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "633_Jo19WXin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syns = wordnet.synsets(\"computer\")\n",
        "print(syns[0].name())"
      ],
      "metadata": {
        "id": "JW1Wj6LD458m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebfddbe-0e7d-4a84-a2aa-71fff4c33e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer.n.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just the word:"
      ],
      "metadata": {
        "id": "LXHuQSpVZy9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(syns[0].lemmas()[0].name())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZNzLV34ZuTe",
        "outputId": "6fb8c86d-db82-49c6-e83c-85d3d90253b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of that first synset:"
      ],
      "metadata": {
        "id": "MGcBT1mXaEiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(syns[0].definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDAq4SZQZ-6J",
        "outputId": "96c3ad5a-ffbe-457b-b115-1507b79b2810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a machine for performing calculations automatically\n"
          ]
        }
      ]
    }
  ]
}